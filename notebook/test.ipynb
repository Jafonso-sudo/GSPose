{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # noqa\n",
    "import sys  # noqa\n",
    "\n",
    "proj_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(proj_root)\n",
    "\n",
    "OBJ_NAME = \"mustard_bottle\"\n",
    "VIDEO_NAME = \"mustard0\"\n",
    "\n",
    "\n",
    "video_dir = os.path.join(proj_root, \"data\", \"inputs\", VIDEO_NAME)\n",
    "tracker_result_video = os.path.join(video_dir)\n",
    "poses_dir = os.path.join(video_dir, \"annotated_poses\")\n",
    "video_gt_mask_dir = os.path.join(video_dir, \"gt_mask\")\n",
    "video_mask_dir = os.path.join(video_dir, \"masks\")\n",
    "video_rgb_dir = os.path.join(video_dir, \"rgb\")\n",
    "video_img_dir = os.path.join(video_dir, \"img\")\n",
    "video_gt_coords_dir = os.path.join(video_dir, \"gt_coords.npy\")\n",
    "video_gt_visibility_dir = os.path.join(video_dir, \"gt_visibility.npy\")\n",
    "obj_dir = os.path.join(proj_root, \"data\", \"objects\", OBJ_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/miniconda3/envs/gspose/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "from posingpixels.utils.offscreen_renderer import ModelRendererOffscreen\n",
    "import cv2\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import matplotlib.pyplot as plt\n",
    "from posingpixels.utils.cotracker import sample_support_grid_points\n",
    "from posingpixels.utils.geometry import interpolate_poses\n",
    "\n",
    "from posingpixels.utils.meshes import get_diameter_from_mesh\n",
    "from posingpixels.alignment import get_safe_query_points\n",
    "from posingpixels.segmentation import segment\n",
    "import torch\n",
    "from cotracker.utils.visualizer import Visualizer\n",
    "from posingpixels.cotracker import get_offline_cotracker_predictions\n",
    "from posingpixels.cotracker import get_online_cotracker_predictions\n",
    "from typing import Optional, Tuple\n",
    "from posingpixels.segmentation import get_bbox_from_mask, process_image_crop\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class YCBinEOATDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, video_dir: str, object_dir: str):\n",
    "        # Video\n",
    "        self.video_dir = video_dir\n",
    "        self.video_rgb_dir = os.path.join(self.video_dir, \"rgb\")\n",
    "        self.rgb_video_files = sorted(glob.glob(f\"{self.video_dir}/rgb/*.png\"))\n",
    "        self.gt_pose_dir = os.path.join(self.video_dir, \"annotated_poses\")\n",
    "        self.gt_pose_files = sorted(glob.glob(f\"{self.video_dir}/annotated_poses/*\"))\n",
    "        self.gt_mask_files = sorted(glob.glob(f\"{self.video_dir}/gt_mask/*\"))\n",
    "\n",
    "        self.K = np.loadtxt(os.path.join(self.video_dir, \"cam_K.txt\")).reshape(3, 3)\n",
    "        self.H, self.W = cv2.imread(self.rgb_video_files[0], cv2.IMREAD_COLOR).shape[:2]\n",
    "\n",
    "        # Segmentation\n",
    "        self.videoname_to_sam_prompt = {\n",
    "            \"mustard0\": [(124, 292), (135, 304), (156, 336)]\n",
    "        }\n",
    "        self.masks_dir = os.path.join(self.video_dir, \"masks\")\n",
    "        if not os.path.exists(self.masks_dir) or len(os.listdir(self.masks_dir)) == 0:\n",
    "            segment(\n",
    "                self.video_rgb_dir,\n",
    "                self.masks_dir,\n",
    "                prompts=self.videoname_to_sam_prompt[self.video_name],\n",
    "            )\n",
    "        self.mask_files = sorted(glob.glob(f\"{self.masks_dir}/*.png\"))\n",
    "\n",
    "        # Object\n",
    "        self.object_dir = object_dir\n",
    "        self.obj_path = os.path.join(self.object_dir, \"textured_simple.obj\")\n",
    "        mesh = self.get_mesh()\n",
    "        self.obj_diameter = get_diameter_from_mesh(mesh)\n",
    "        self.renderer = ModelRendererOffscreen(self.K, self.H, self.W)\n",
    "\n",
    "        # Both\n",
    "        self.videoname_to_object = {\n",
    "            \"bleach0\": \"021_bleach_cleanser\",\n",
    "            \"bleach_hard_00_03_chaitanya\": \"021_bleach_cleanser\",\n",
    "            \"cracker_box_reorient\": \"003_cracker_box\",\n",
    "            \"cracker_box_yalehand0\": \"003_cracker_box\",\n",
    "            \"mustard0\": \"006_mustard_bottle\",\n",
    "            \"mustard_easy_00_02\": \"006_mustard_bottle\",\n",
    "            \"sugar_box1\": \"004_sugar_box\",\n",
    "            \"sugar_box_yalehand0\": \"004_sugar_box\",\n",
    "            \"tomato_soup_can_yalehand0\": \"005_tomato_soup_can\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def video_name(self):\n",
    "        return os.path.basename(self.video_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_video_files)\n",
    "\n",
    "    def get_mesh(self) -> trimesh.Trimesh:\n",
    "        return trimesh.load_mesh(self.obj_path)\n",
    "\n",
    "    @property\n",
    "    def image_size(self):\n",
    "        if self.H is None or self.W is None:\n",
    "            self.H, self.W = cv2.imread(\n",
    "                os.listdir(self.video_rgb_dir)[0], cv2.IMREAD_COLOR\n",
    "            ).shape[:2]\n",
    "        return self.H, self.W\n",
    "\n",
    "    def get_gt_poses(self) -> np.ndarray:\n",
    "        pose = None\n",
    "        poses = []\n",
    "        for i in range(len(self)):\n",
    "            pose_i = self.get_gt_pose(i)\n",
    "            pose = pose_i if pose_i is not None else pose\n",
    "            poses.append(pose)\n",
    "        return np.array(poses)\n",
    "\n",
    "    def get_gt_pose(self, idx: int) -> Optional[np.ndarray]:\n",
    "        file = os.path.join(self.gt_pose_dir, f\"{idx:07d}.txt\")\n",
    "        if not os.path.exists(file):\n",
    "            return None\n",
    "        return np.loadtxt(file).reshape(4, 4)\n",
    "\n",
    "    def get_rgb(self, idx: int) -> np.ndarray:\n",
    "        return cv2.cvtColor(\n",
    "            cv2.imread(self.rgb_video_files[idx], cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB\n",
    "        )\n",
    "\n",
    "    def get_mask(self, idx: int) -> np.ndarray:\n",
    "        return cv2.imread(self.mask_files[idx], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    def get_gt_mask(self, idx: int) -> np.ndarray:\n",
    "        return cv2.imread(self.gt_mask_files[idx], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    def render_mesh_at_pose(\n",
    "        self, pose: Optional[np.ndarray] = None, idx: Optional[int] = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        assert (pose is None) != (idx is None)\n",
    "        pose = self.get_gt_pose(idx) if pose is None else pose\n",
    "        return self.renderer.render(pose, self.get_mesh())\n",
    "\n",
    "    def get_canonical_pose(self):\n",
    "        canonical_pose = np.eye(4)\n",
    "        diameter = self.obj_diameter\n",
    "\n",
    "        # Translate along z-axis by diameter\n",
    "        canonical_pose[:3, 3] = np.array([0, 0, diameter])\n",
    "        # Rotate 90 degrees around x-axis then rotate around y-axis 180 degrees\n",
    "        canonical_pose[:3, :3] = np.array([[-1, 0, 0], [0, 0, -1], [0, -1, 0]])\n",
    "\n",
    "        return canonical_pose\n",
    "\n",
    "\n",
    "class CoMeshTracker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: YCBinEOATDataset,\n",
    "        visible_background: bool = False,\n",
    "        crop: bool = True,\n",
    "        offline: bool = True,\n",
    "        offline_limit: int = 500,\n",
    "        support_grid: Optional[int] = None,\n",
    "        interpolation_steps: int = 15,\n",
    "        mask_threshold: float = 0.5,\n",
    "        device: torch.device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        ),\n",
    "    ):\n",
    "        assert crop != (\n",
    "            support_grid is not None\n",
    "        ), \"SUPPORT_GRID must be set if BLACK_BACKGROUND is False (and vice versa)\"\n",
    "        # Model Config\n",
    "        self.visible_background = visible_background\n",
    "        self.crop = crop\n",
    "        self.offline = offline\n",
    "        self.offline_limit = offline_limit\n",
    "        self.limit = offline_limit if offline else len(self)\n",
    "        self.support_grid = support_grid\n",
    "        self.interpolation_steps = interpolation_steps\n",
    "        self.mask_threshold = mask_threshold\n",
    "        self.model_resolution = (384, 512)\n",
    "        self.device = device\n",
    "        # Dataset Config\n",
    "        self.dataset = dataset\n",
    "        self.K = dataset.K\n",
    "        self.H, self.W = dataset.image_size\n",
    "        # Initialization Config\n",
    "        self.start_pose = dataset.get_gt_pose(0)\n",
    "        self.base_pose = (\n",
    "            dataset.get_canonical_pose()\n",
    "            if self.interpolation_steps > 1\n",
    "            else self.start_pose\n",
    "        )\n",
    "        self.query_poses = [self.base_pose]\n",
    "        self.init_video_dir = os.path.join(self.dataset.video_dir, \"init_video\")\n",
    "        self.cotracker_input_dir = os.path.join(self.dataset.video_dir, \"input\")\n",
    "\n",
    "    def __call__(self):\n",
    "        # Create init video\n",
    "        self.create_init_video()\n",
    "        # Prepare img directory (input for CoTracker)\n",
    "        self.prepare_img_directory()\n",
    "        # Prepare query points\n",
    "        self.get_query_points()\n",
    "        # Run CoTracker\n",
    "        return self.run_cotracker()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) + self.interpolation_steps\n",
    "\n",
    "    def get_rgb(self, idx: int) -> np.ndarray:\n",
    "        if idx < self.interpolation_steps:\n",
    "            return cv2.imread(\n",
    "                os.path.join(self.init_video_dir, f\"{idx:05d}.jpg\"), cv2.IMREAD_COLOR\n",
    "            )\n",
    "        return self.dataset.get_rgb(idx - self.interpolation_steps)\n",
    "\n",
    "    def get_mask(self, idx: int) -> np.ndarray:\n",
    "        if idx < self.interpolation_steps:\n",
    "            return (\n",
    "                cv2.imread(\n",
    "                    os.path.join(self.init_video_dir, f\"{idx:05d}.png\"),\n",
    "                    cv2.IMREAD_GRAYSCALE,\n",
    "                )\n",
    "                / 255\n",
    "            )\n",
    "        return self.dataset.get_gt_mask(idx - self.interpolation_steps)\n",
    "\n",
    "    def get_gt_poses(self) -> np.ndarray:\n",
    "        return np.concatenate([self.interpolation_poses, self.dataset.get_gt_poses()])\n",
    "\n",
    "    def get_gt_pose(self, idx: int) -> Optional[np.ndarray]:\n",
    "        if idx < self.interpolation_steps:\n",
    "            return self.interpolation_poses[idx]\n",
    "        return self.dataset.get_gt_pose(idx - self.interpolation_steps)\n",
    "\n",
    "    def prepare_img_directory(self):\n",
    "        # Clear directory\n",
    "        if not os.path.exists(self.cotracker_input_dir):\n",
    "            os.makedirs(self.cotracker_input_dir)\n",
    "        for f in os.listdir(self.cotracker_input_dir):\n",
    "            os.remove(os.path.join(self.cotracker_input_dir, f))\n",
    "        # Prepare images for CoTracker\n",
    "        self.bboxes, self.scaling = [], []\n",
    "        for i in tqdm(range(self.limit), desc=\"Preparing images for CoTracker\"):\n",
    "            rgb = self.get_rgb(i)\n",
    "            mask = self.get_mask(i) > self.mask_threshold\n",
    "            if not self.visible_background:\n",
    "                rgb[mask == 0, :] = 0\n",
    "            if self.crop:\n",
    "                bbox = get_bbox_from_mask(mask)\n",
    "                assert bbox\n",
    "                rgb, processed_bbox, scaling_factor = process_image_crop(\n",
    "                    rgb,\n",
    "                    bbox,\n",
    "                    padding=10,\n",
    "                    target_size=self.model_resolution,\n",
    "                )\n",
    "                self.bboxes.append(processed_bbox)\n",
    "                self.scaling.append(scaling_factor)\n",
    "\n",
    "            cv2.imwrite(\n",
    "                os.path.join(self.cotracker_input_dir, f\"{i:05d}.jpg\"),\n",
    "                cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR),\n",
    "            )\n",
    "\n",
    "    def create_init_video(self):\n",
    "        assert (\n",
    "            len(self.query_poses) == 1\n",
    "        ), \"Not yet implemented for multiple query poses\"\n",
    "        assert self.base_pose is not None and self.start_pose is not None\n",
    "\n",
    "        if not self.interpolation_steps:\n",
    "            return None, None, None\n",
    "        if not os.path.exists(self.init_video_dir):\n",
    "            os.makedirs(self.init_video_dir)\n",
    "        for f in os.listdir(self.init_video_dir):\n",
    "            os.remove(os.path.join(self.init_video_dir, f))\n",
    "\n",
    "        self.interpolation_poses = interpolate_poses(\n",
    "            self.base_pose[:3, :3],\n",
    "            self.base_pose[:3, 3],\n",
    "            self.start_pose[:3, :3],\n",
    "            self.start_pose[:3, 3],\n",
    "            self.interpolation_steps,\n",
    "        )\n",
    "\n",
    "        base_frame = self.dataset.get_rgb(0)\n",
    "        for i, P_i in enumerate(self.interpolation_poses):\n",
    "            rgb, depth = self.dataset.render_mesh_at_pose(pose=P_i)\n",
    "            depth_rgb = depth[:, :, None]\n",
    "            rgb = base_frame * (depth_rgb <= 0) + rgb * (depth_rgb > 0)\n",
    "\n",
    "            # Save RGB and Mask\n",
    "            cv2.imwrite(\n",
    "                os.path.join(self.init_video_dir, f\"{i:05d}.jpg\"),\n",
    "                cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR),\n",
    "            )\n",
    "            cv2.imwrite(\n",
    "                os.path.join(self.init_video_dir, f\"{i:05d}.png\"),\n",
    "                (depth > 0).astype(np.uint8) * 255,\n",
    "            )\n",
    "\n",
    "    def get_query_points(self):\n",
    "        assert (\n",
    "            len(self.query_poses) == 1\n",
    "        ), \"Not yet implemented for multiple query poses\"\n",
    "        assert self.base_pose is not None\n",
    "        # Get query points\n",
    "        self.unposed_3d_points, self.query_2d_points = get_safe_query_points(\n",
    "            R=self.base_pose[:3, :3],\n",
    "            T=self.base_pose[:3, 3],\n",
    "            camK=self.K,\n",
    "            H=self.H,\n",
    "            W=self.W,\n",
    "            mesh=self.dataset.get_mesh(),\n",
    "            min_pixel_distance=10 if not self.interpolation_steps else 25,\n",
    "            alpha_margin=5 if not self.interpolation_steps else 15,\n",
    "            depth_margin=2 if not self.interpolation_steps else 6,\n",
    "        )\n",
    "        self.object_query_points_num = len(self.unposed_3d_points)\n",
    "        # Add support grid\n",
    "        if self.support_grid is not None:\n",
    "            support_grid_points = sample_support_grid_points(\n",
    "                self.H,\n",
    "                self.W,\n",
    "                self.interpolation_steps,\n",
    "                self.get_mask(0),\n",
    "                grid_size=self.support_grid,\n",
    "            )\n",
    "            self.query_2d_points = np.concatenate(\n",
    "                [self.query_2d_points, support_grid_points], axis=0\n",
    "            )\n",
    "        # Prepare query points for CoTracker\n",
    "        self.input_query_points = self.query_2d_points.copy()\n",
    "        if self.crop:\n",
    "            self.input_query_points[:, 1] -= self.bboxes[0][0]\n",
    "            self.input_query_points[:, 2] -= self.bboxes[0][1]\n",
    "            self.input_query_points[:, 1] *= self.scaling[0][0]\n",
    "            self.input_query_points[:, 2] *= self.scaling[0][1]\n",
    "\n",
    "    def prepare_cotracker_initialization(self):\n",
    "        pass\n",
    "\n",
    "    def run_cotracker(self):\n",
    "        if not self.offline:\n",
    "            return get_online_cotracker_predictions(\n",
    "                self.cotracker_input_dir,\n",
    "                grid_size=0,\n",
    "                queries=self.input_query_points,\n",
    "            )\n",
    "        else:\n",
    "            return get_offline_cotracker_predictions(\n",
    "                self.cotracker_input_dir,\n",
    "                grid_size=0,\n",
    "                queries=self.input_query_points,\n",
    "                limit=self.offline_limit,\n",
    "            )\n",
    "\n",
    "\n",
    "dataset = YCBinEOATDataset(video_dir, obj_dir)\n",
    "tracker = CoMeshTracker(dataset, offline_limit=500, interpolation_steps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing images for CoTracker:  79%|███████▉  | 395/500 [00:09<00:02, 38.33it/s]"
     ]
    }
   ],
   "source": [
    "pred_tracks, pred_visibility, pred_confidence = tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from posingpixels.datasets import load_video_images\n",
    "\n",
    "\n",
    "video = load_video_images(\n",
    "    video_img_dir, limit=tracker.offline_limit if tracker.offline else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to /home/joao/Documents/repositories/GSPose/data/inputs/mustard0/video.mp4\n"
     ]
    }
   ],
   "source": [
    "def visualize_results(\n",
    "    video,\n",
    "    pred_tracks,\n",
    "    pred_visibility,\n",
    "    pred_confidence,\n",
    "    save_dir,\n",
    "    num_of_main_queries=None,\n",
    "    filename=\"video\",\n",
    "):\n",
    "    if num_of_main_queries is None:\n",
    "        num_of_main_queries = pred_tracks.shape[2]\n",
    "    vis = Visualizer(save_dir=save_dir, pad_value=0, linewidth=3)\n",
    "    vis.visualize(\n",
    "        video,\n",
    "        pred_tracks[:, :, :num_of_main_queries, :],\n",
    "        (pred_visibility * pred_confidence > 0.6)[:, :, :num_of_main_queries],\n",
    "        filename=filename,\n",
    "    )\n",
    "\n",
    "\n",
    "visualize_results(\n",
    "    video,\n",
    "    pred_tracks,\n",
    "    pred_visibility,\n",
    "    pred_confidence,\n",
    "    tracker_result_video,\n",
    "    num_of_main_queries=tracker.object_query_points_num,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 30/30 [00:20<00:00,  1.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate ground truth for the coords location (& visibility)\n",
    "from typing import Tuple\n",
    "\n",
    "from posingpixels.utils.cotracker import get_ground_truths\n",
    "\n",
    "\n",
    "N = len(tracker) if not tracker.offline else tracker.offline_limit\n",
    "# _, depths = tracker.dataset.renderer.render_batch(tracker.get_gt_poses(), tracker.dataset.get_mesh())\n",
    "\n",
    "gt_coords = np.zeros((N, tracker.object_query_points_num, 2))\n",
    "gt_visibility = np.zeros((N, tracker.object_query_points_num))\n",
    "\n",
    "poses = tracker.get_gt_poses()\n",
    "for i in tqdm(range(N), desc=\"Processing frames\"):\n",
    "    mask = tracker.get_mask(i)\n",
    "    pose = poses[i]\n",
    "    depth = tracker.dataset.render_mesh_at_pose(pose=pose)[1]\n",
    "    gt_coords[i], gt_visibility[i] = get_ground_truths(\n",
    "        pose, tracker.K, tracker.unposed_3d_points, mask, depth\n",
    "    )\n",
    "np.save(video_gt_coords_dir, gt_coords)\n",
    "np.save(video_gt_visibility_dir, gt_visibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to /home/joao/Documents/repositories/GSPose/data/inputs/mustard0/gt_video.mp4\n"
     ]
    }
   ],
   "source": [
    "from posingpixels.utils.cotracker import scale_by_crop\n",
    "\n",
    "\n",
    "torch_bbox = torch.tensor(tracker.bboxes).to(device)[:N]\n",
    "torch_scaling = torch.tensor(tracker.scaling).to(device)[:N]\n",
    "gt_coords_torch = scale_by_crop(\n",
    "    torch.tensor(gt_coords).to(device)[:N], torch_bbox, torch_scaling\n",
    ")[None]\n",
    "gt_visibility_torch = torch.tensor(gt_visibility).to(device)[None]\n",
    "visualize_results(\n",
    "    video,\n",
    "    gt_coords_torch,\n",
    "    gt_visibility_torch,\n",
    "    torch.ones_like(gt_visibility_torch).to(device),\n",
    "    tracker_result_video,\n",
    "    num_of_main_queries=tracker.object_query_points_num,\n",
    "    filename=\"gt_video\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See distribution of confidence values over time\n",
    "conf_np = pred_confidence.detach().cpu().numpy()[0]\n",
    "print(conf_np.shape)\n",
    "plt.plot(conf_np.mean(axis=1))\n",
    "plt.axvline(x=tracker.interpolation_steps - 1, color=\"r\", linestyle=\"--\")\n",
    "plt.title(\"Mean confidence over time\")\n",
    "plt.show()\n",
    "plt.plot(conf_np.mean(axis=1)[:20])\n",
    "# Plot vertical line at tracker.interpolation_steps\n",
    "plt.axvline(x=tracker.interpolation_steps - 1, color=\"r\", linestyle=\"--\")\n",
    "plt.title(\"Mean confidence over time\")\n",
    "plt.show()\n",
    "# Plot distribution of confidence values at frame tracker.interpolation_steps - 1\n",
    "plt.hist(conf_np[tracker.interpolation_steps - 1])\n",
    "plt.title(\n",
    "    f\"Distribution of confidence values at frame {tracker.interpolation_steps - 1}\"\n",
    ")\n",
    "plt.show()\n",
    "# Plot distribution of confidence values at frame tracker.interpolation_steps + 1\n",
    "plt.hist(conf_np[tracker.interpolation_steps + 1])\n",
    "plt.title(\n",
    "    f\"Distribution of confidence values at frame {tracker.interpolation_steps + 1}\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gspose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
