{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # noqa\n",
    "import sys  # noqa\n",
    "\n",
    "proj_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(proj_root)\n",
    "\n",
    "OBJ_NAME = \"mustard_bottle\"\n",
    "VIDEO_NAME = \"mustard0\"\n",
    "\n",
    "VIS_CONF_THRESHOLD = 0.9\n",
    "\n",
    "\n",
    "video_dir = os.path.join(proj_root, \"data\", \"inputs\", VIDEO_NAME)\n",
    "tracker_result_video = os.path.join(video_dir)\n",
    "obj_dir = os.path.join(proj_root, \"data\", \"objects\", OBJ_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/miniconda3/envs/gspose/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from posingpixels.datasets import YCBinEOATDataset, load_video_images\n",
    "from posingpixels.utils.cotracker import visualize_results\n",
    "from posingpixels.utils.evaluation import get_gt_tracks\n",
    "from posingpixels.pnp import GradientPnP\n",
    "\n",
    "\n",
    "from posingpixels.utils.cotracker import unscale_by_crop\n",
    "\n",
    "from posingpixels.utils.evaluation import compute_add_metrics\n",
    "\n",
    "from posingpixels.pointselector import SelectMostConfidentPoint\n",
    "from posingpixels.utils.evaluation import compute_tapvid_metrics\n",
    "\n",
    "\n",
    "import mediapy\n",
    "from posingpixels.utils.geometry import (\n",
    "    apply_pose_to_points_batch,\n",
    "    render_points_in_2d_batch,\n",
    ")\n",
    "from posingpixels.visualization import overlay_bounding_box_on_video\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from posingpixels.alignment import CanonicalPointSampler\n",
    "from posingpixels.cotracker import CropCoPoseTracker\n",
    "from posingpixels.pnp import OpenCVePnP\n",
    "from posingpixels.cotracker import CoTrackerInput\n",
    "from posingpixels.pointselector import SelectMostConfidentView\n",
    "from posingpixels.utils.cotracker import get_ground_truths\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using CAD mask for YCBinEOATDataset. This is fine if being used to create a Gaussian Splat of the CAD, but not for running point tracker.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/joao/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/joao/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS indices:  tensor([  0, 502, 103, 564,  91, 582, 112, 548], device='cuda:0')\n",
      "[100/737], 13-14:54:27\n",
      "[200/737], 13-14:54:30\n",
      "[300/737], 13-14:54:32\n",
      "[400/737], 13-14:54:35\n",
      "[500/737], 13-14:54:38\n",
      "[600/737], 13-14:54:41\n",
      "[700/737], 13-14:54:43\n",
      "Creating 3D-OGS model for mustard_bottle \n",
      "Output folder: /home/joao/Documents/repositories/GSPose/data/objects/mustard_bottle\n",
      "Reading 737  training image ...\n",
      "737 training samples\n",
      "-----------------------------------------\n",
      "4 testing samples\n",
      "----------------------------------------\n",
      "Loading Training Cameras\n",
      "Loading Test Cameras\n",
      "Number of points at initialisation :  4350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3DGO modeling progress:  23%|██▎       | 6830/30000 [06:04<03:06, 124.27it/s, Loss=nan]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m YCBinEOATDataset(video_dir, obj_dir, use_cad_rgb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_cad_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m model_net \u001b[38;5;241m=\u001b[39m load_model_net(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(proj_root, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/model_weights.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m ref_database \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_or_load_gaussian_splat_from_ycbineoat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/repositories/GSPose/posingpixels/utils/gs_pose.py:121\u001b[0m, in \u001b[0;36mcreate_or_load_gaussian_splat_from_ycbineoat\u001b[0;34m(dataset, model_net, device)\u001b[0m\n\u001b[1;32m    118\u001b[0m gs_modelData\u001b[38;5;241m.\u001b[39mreferloader \u001b[38;5;241m=\u001b[39m dataset\n\u001b[1;32m    119\u001b[0m gs_modelData\u001b[38;5;241m.\u001b[39mqueryloader \u001b[38;5;241m=\u001b[39m dataset\n\u001b[0;32m--> 121\u001b[0m obj_gaussians \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_3D_Gaussian_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgs_modelData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgs_optimData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgs_pipeData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_gaussian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m reference_database[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_gaussians_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_database_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/3DGO_model.ply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _key, _val \u001b[38;5;129;01min\u001b[39;00m reference_database\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Documents/repositories/GSPose/gaussian_object/build_3DGaussianObject.py:203\u001b[0m, in \u001b[0;36mcreate_3D_Gaussian_object\u001b[0;34m(dataset, opt, pipe, testing_iterations, saving_iterations, checkpoint_iterations, checkpoint, debug_from, return_gaussian)\u001b[0m\n\u001b[1;32m    200\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Log and save\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m training_report(\n\u001b[1;32m    204\u001b[0m     tb_writer,\n\u001b[1;32m    205\u001b[0m     iteration,\n\u001b[1;32m    206\u001b[0m     Ll1,\n\u001b[1;32m    207\u001b[0m     loss,\n\u001b[1;32m    208\u001b[0m     l1_loss,\n\u001b[1;32m    209\u001b[0m     iter_start\u001b[38;5;241m.\u001b[39melapsed_time(iter_end),\n\u001b[1;32m    210\u001b[0m     testing_iterations,\n\u001b[1;32m    211\u001b[0m     scene,\n\u001b[1;32m    212\u001b[0m     render,\n\u001b[1;32m    213\u001b[0m     (pipe, background),\n\u001b[1;32m    214\u001b[0m )\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m saving_iterations:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[ITER \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] Saving Gaussians\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(iteration))\n",
      "File \u001b[0;32m~/Documents/repositories/GSPose/gaussian_object/build_3DGaussianObject.py:322\u001b[0m, in \u001b[0;36mtraining_report\u001b[0;34m(tb_writer, iteration, Ll1, loss, l1_loss, elapsed, testing_iterations, scene, renderFunc, renderArgs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tb_writer:\n\u001b[1;32m    321\u001b[0m     tb_writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss_patches/l1_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, Ll1\u001b[38;5;241m.\u001b[39mitem(), iteration)\n\u001b[0;32m--> 322\u001b[0m     tb_writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss_patches/total_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, iteration)\n\u001b[1;32m    323\u001b[0m     tb_writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, elapsed, iteration)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# Report test and samples of training set\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from inference import create_reference_database_from_RGB_images\n",
    "from posingpixels.utils.gs_pose import create_or_load_gaussian_splat_from_ycbineoat, load_model_net\n",
    "\n",
    "\n",
    "dataset = YCBinEOATDataset(video_dir, obj_dir, use_cad_rgb=True, use_cad_mask=True)\n",
    "model_net = load_model_net(os.path.join(proj_root, \"checkpoints/model_weights.pth\"))\n",
    "\n",
    "ref_database = create_or_load_gaussian_splat_from_ycbineoat(dataset, model_net, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from posingpixels.utils.gs_pose import render_gaussian_model, render_gaussian_model_with_info\n",
    "from posingpixels.visualization import get_gaussian_splat_pointcloud, plot_pointclouds\n",
    "\n",
    "frame_idx = 0\n",
    "\n",
    "gaussian_object = ref_database[\"obj_gaussians\"]\n",
    "\n",
    "object_pointcloud = get_gaussian_splat_pointcloud(gaussian_object)\n",
    "plot_pointclouds({OBJ_NAME.capitalize(): object_pointcloud}, \"Gaussian Object\")\n",
    "\n",
    "initial_pose = dataset.get_gt_pose(frame_idx)\n",
    "initial_R, initial_T = initial_pose[:3, :3], initial_pose[:3, 3]\n",
    "\n",
    "render = render_gaussian_model_with_info(\n",
    "    gaussian_object, dataset.K, dataset.H, dataset.W, R=initial_R, T=initial_T\n",
    ")\n",
    "\n",
    "mask = dataset.get_mask(frame_idx) > 0.7\n",
    "x1, y1, x2, y2 = np.min(np.where(mask)[1]) - 5, np.min(np.where(mask)[0]) - 5, np.max(np.where(mask)[1]) + 5, np.max(np.where(mask)[0]) + 5\n",
    "\n",
    "image = render['image'][y1:y2, x1:x2]\n",
    "alpha = render['alpha'][0, y1:y2, x1:x2].detach().cpu().numpy()\n",
    "depth = render['depth'][0, y1:y2, x1:x2].detach().cpu().numpy()\n",
    "cad_depth = dataset.get_cad_depth(frame_idx)[y1:y2, x1:x2]\n",
    "mask = dataset.get_mask(frame_idx)[y1:y2, x1:x2] > 0.7\n",
    "rgb = dataset.get_rgb(frame_idx)[y1:y2, x1:x2]\n",
    "\n",
    "image_high_alpha = image.copy()\n",
    "image_high_alpha[alpha < 0.9] = 0\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(\"Gaussian Splat Rendered\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(image_high_alpha)\n",
    "# plt.imshow(alpha > 0.99, alpha=0.5)\n",
    "plt.title(\"Gaussian Splat Rendered, alpha > 0.9\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(cad_depth)\n",
    "plt.imshow(rgb, alpha=0.5)\n",
    "plt.title(\"CAD Depth\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(cad_depth > 0)\n",
    "plt.imshow(mask, alpha=0.5)\n",
    "plt.title(\"CAD Mask vs Actual Mask\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.imshow(mask, alpha=0.5)\n",
    "plt.title(\"Actual Mask vs Rendered Splat (Showcasing Ghost Effect)\")\n",
    "plt.show()\n",
    "\n",
    "# plt.imshow(alpha > 0.99)\n",
    "# plt.show()\n",
    "# plt.imshow(depth)\n",
    "# plt.show()\n",
    "\n",
    "# # Overlap alpha with image 50% transparency\n",
    "# plt.imshow(image)\n",
    "# plt.imshow(alpha > 0.99, alpha=0.7)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnp_solver = OpenCVePnP(min_inliers=20, ransac_inliner_threshold=2.0)\n",
    "# tracker = CoMeshTracker(\n",
    "#     dataset,\n",
    "#     None,\n",
    "#     # support_grid=10,\n",
    "#     offline=False,\n",
    "#     # crop=False,\n",
    "#     # visible_background=True,\n",
    "#     # downcast=True,\n",
    "#     # better_initialization=False,\n",
    "#     # limit=100,\n",
    "#     # interpolation_steps=80,\n",
    "#     axis_rotation_steps=40,\n",
    "#     final_interpolation_steps=40,\n",
    "#     query_frames=[0, 10, 20, 30],\n",
    "#     device=device,\n",
    "# )\n",
    "\n",
    "point_sampler = CanonicalPointSampler()\n",
    "tracker = CropCoPoseTracker(\n",
    "    canonical_point_sampler=point_sampler,\n",
    "    # pnp_solver=pnp_solver,\n",
    "    pose_interpolation_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reset_frame_range()\n",
    "with torch.no_grad():\n",
    "    pred_tracks, pred_visibility, pred_confidence, pred_tracks_original, tracker_input = (\n",
    "        tracker(dataset)\n",
    "    )\n",
    "Q = tracker_input.num_query_points\n",
    "N = len(tracker_input)\n",
    "# Pickle the results\n",
    "import pickle\n",
    "\n",
    "with open(os.path.join(video_dir, \"tracker_results.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(\n",
    "        {\n",
    "            \"pred_tracks\": pred_tracks,\n",
    "            \"pred_visibility\": pred_visibility,\n",
    "            \"pred_confidence\": pred_confidence,\n",
    "            \"pred_tracks_original\": pred_tracks_original,\n",
    "            \"N\": N,\n",
    "            \"Q\": Q,\n",
    "            \"bboxes\": tracker_input.bboxes[tracker_input.prepend_length:],\n",
    "            \"scaling\": tracker_input.scaling[tracker_input.prepend_length:],\n",
    "            \"video\": load_video_images(tracker_input.video_dir)[:, tracker_input.prepend_length:],\n",
    "        },\n",
    "        f,\n",
    "    )\n",
    "    \n",
    "# Load the results\n",
    "import pickle\n",
    "\n",
    "with open(os.path.join(video_dir, \"tracker_results.pkl\"), \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "    pred_tracks = results[\"pred_tracks\"]\n",
    "    pred_visibility = results[\"pred_visibility\"]\n",
    "    pred_confidence = results[\"pred_confidence\"]\n",
    "    pred_tracks_original = results[\"pred_tracks_original\"]\n",
    "    N = results[\"N\"]\n",
    "    Q = results[\"Q\"]\n",
    "    bboxes = results[\"bboxes\"]\n",
    "    scaling = results[\"scaling\"]\n",
    "    video = results[\"video\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best(\n",
    "    tracker_input: CoTrackerInput, pred_tracks, pred_visibility, pred_confidence, view=False\n",
    "):\n",
    "    true_indexes = torch.tensor(tracker_input.query_to_point_indexes, device=device)\n",
    "    query_lengths = torch.tensor(tracker_input.query_lengths, device=device)\n",
    "\n",
    "    if not view:\n",
    "        point_selector = SelectMostConfidentPoint(\n",
    "            tracker_input.num_canonical_points, true_indexes, query_lengths\n",
    "        )\n",
    "    else:\n",
    "        point_selector = SelectMostConfidentView(\n",
    "            tracker_input.num_canonical_points, true_indexes, query_lengths\n",
    "        )\n",
    "\n",
    "    best_coords, best_vis, best_conf, best_indices = point_selector.query_to_point(\n",
    "        pred_tracks[0],\n",
    "        pred_visibility[0],\n",
    "        pred_confidence[0],\n",
    "        # pred_visibility[0] * pred_confidence[0],\n",
    "    )\n",
    "    best_coords = best_coords.unsqueeze(0)\n",
    "    best_vis = best_vis.unsqueeze(0)\n",
    "    best_conf = best_conf.unsqueeze(0)\n",
    "\n",
    "    best_coords_original = unscale_by_crop(\n",
    "        best_coords[0],\n",
    "        torch.tensor(tracker_input.bboxes).to(device),\n",
    "        torch.tensor(tracker_input.scaling).to(device),\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    return best_coords, best_vis, best_conf, best_coords_original, best_indices\n",
    "\n",
    "\n",
    "def estimate_poses(\n",
    "    tracker_input: CoTrackerInput, best_coords_original, best_vis, best_conf\n",
    "):\n",
    "    N = len(tracker_input)\n",
    "    K = tracker_input.dataset.K\n",
    "    x = (\n",
    "        torch.tensor(tracker_input.canonical_points, dtype=torch.float32)\n",
    "        .to(device)\n",
    "        .unsqueeze(0)\n",
    "        .repeat(N, 1, 1)\n",
    "    )\n",
    "    y = best_coords_original.detach().clone().squeeze(0)[:N]\n",
    "\n",
    "    weights = (best_vis * best_conf).float()[:N]\n",
    "    weights[best_vis * best_conf < VIS_CONF_THRESHOLD] = 0\n",
    "    weights = weights.squeeze(0)\n",
    "\n",
    "    camKs = torch.tensor(K[np.newaxis, :], device=device).float()\n",
    "\n",
    "    epnp_cv_solver = OpenCVePnP(\n",
    "        X=x[0],\n",
    "        K=camKs,\n",
    "    )\n",
    "    epnp_cv_R, epnp_cv_T, err = epnp_cv_solver(\n",
    "        y, X=x, K=torch.tensor(K).to(device).float(), weights=weights\n",
    "    )\n",
    "\n",
    "    epnp_cv_poses = torch.eye(4).to(device).unsqueeze(0).repeat(N, 1, 1)\n",
    "    epnp_cv_poses[:, :3, :3] = epnp_cv_R\n",
    "    epnp_cv_poses[:, :3, 3] = epnp_cv_T\n",
    "    return epnp_cv_poses, err\n",
    "\n",
    "# def improve_poses(\n",
    "#     x, y, K, poses, weights\n",
    "# ) -> torch.Tensor:\n",
    "#     # Initialize the optimizer to the current pose\n",
    "    \n",
    "#     # Try every pose against every frame, take the best (using reprojection error w/ Huber loss)\n",
    "    \n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    (\n",
    "        pred_tracks_batch,\n",
    "        pred_confidence_batch,\n",
    "        pred_visibility_batch,\n",
    "        pred_poses_batch,\n",
    "        pred_poses_err,\n",
    "        best_indices_batch\n",
    "    ) = [], [], [], [], [], []\n",
    "    dataset.reset_frame_range()\n",
    "    start_pose = dataset.get_gt_pose(0)\n",
    "    step = 32\n",
    "    overlap = 0\n",
    "    tracks = vis = conf = track_input = best_coords = best_conf = best_vis = None\n",
    "    for i in range(0, dataset.max_frames, step - overlap):\n",
    "        dataset.start_frame = i\n",
    "        dataset.end_frame = min(i + step, dataset.max_frames)\n",
    "        rgb = dataset.get_rgb(0)\n",
    "        # plt.imshow(rgb)\n",
    "        # plt.show()\n",
    "        print(f\"Processing frames {dataset.start_frame} to {dataset.end_frame}\")\n",
    "        # TODO: Force pose to always be in view, and if it is too far gone, do not include it\n",
    "        # start_pose[:3, 3] = np.array([0, 0, dataset._get_safe_distance()]) # TODO: Doesn't give right perspective, but ensures it's always in view\n",
    "        rgb, depth, _ = dataset.render_mesh_at_pose(start_pose)\n",
    "        # TODO: It's good to initialize every point (maybe with a confidence penalty for the ones in the dynamic template)\n",
    "        # Initialzie dynamic ones from the ones in the best_coords for that point (same with conf and vis)\n",
    "        if tracks is not None and overlap > 0:\n",
    "            assert (\n",
    "                vis is not None\n",
    "                and conf is not None\n",
    "                and track_input is not None\n",
    "                and best_coords is not None\n",
    "                and best_conf is not None\n",
    "                and best_vis is not None\n",
    "            )\n",
    "            last_specific_length = track_input.query_lengths[-1] if len(track_input.query_lengths) > 4 else -tracks.shape[2]\n",
    "            forced_coords = tracks[:, -overlap:, :-last_specific_length]\n",
    "            forced_vis = vis[:, -overlap:, :-last_specific_length]\n",
    "            forced_vis = torch.logit(forced_vis).clamp(-tracker.init_value, tracker.init_value)\n",
    "            forced_conf = conf[:, -overlap:, :-last_specific_length]\n",
    "            forced_conf = torch.logit(forced_conf).clamp(-tracker.init_value, tracker.init_value)\n",
    "        else:\n",
    "            forced_coords = forced_vis = forced_conf = None\n",
    "        print(start_pose)\n",
    "        tracks, vis, conf, tracks_original, track_input = tracker(\n",
    "            dataset,\n",
    "            start_pose=start_pose,\n",
    "            query_poses=start_pose[np.newaxis],\n",
    "            forced_coords=forced_coords,\n",
    "            forced_vis=forced_vis,\n",
    "            forced_conf=forced_conf,\n",
    "        )\n",
    "\n",
    "        Q = track_input.num_query_points\n",
    "        N = len(track_input)\n",
    "\n",
    "        best_coords, best_vis, best_conf, best_coords_original, best_indices = (\n",
    "            choose_best(track_input, tracks, vis, conf, view=True)\n",
    "        )\n",
    "        best_indices_batch.append(best_indices[track_input.prepend_length :])\n",
    "        \n",
    "\n",
    "        poses, err = estimate_poses(track_input, best_coords_original, best_vis, best_conf)\n",
    "        poses = poses[\n",
    "            track_input.prepend_length :\n",
    "        ]\n",
    "        if err is not None:\n",
    "            err = err[\n",
    "                track_input.prepend_length :\n",
    "            ]\n",
    "        start_pose = poses[-1].detach().cpu().numpy()\n",
    "        \n",
    "        video = load_video_images(track_input.video_dir, limit=N)\n",
    "        visualize_results(\n",
    "            video,\n",
    "            tracks,\n",
    "            vis,\n",
    "            conf,\n",
    "            tracker_result_video + f\"_{i}\",\n",
    "            num_of_main_queries=track_input.num_query_points,\n",
    "        )\n",
    "        \n",
    "        visualize_results(\n",
    "            video,\n",
    "            best_coords,\n",
    "            best_vis,\n",
    "            best_conf,\n",
    "            tracker_result_video + f\"_{i}_best\",\n",
    "            num_of_main_queries=track_input.num_canonical_points,\n",
    "        )\n",
    "        \n",
    "        best_coords, best_vis, best_conf, best_coords_original, best_indices = (\n",
    "            choose_best(track_input, tracks, vis, conf, view=False)\n",
    "        )\n",
    "\n",
    "\n",
    "        print(tracks.shape, vis.shape, conf.shape)\n",
    "        tracks = tracks[:, track_input.prepend_length :]\n",
    "        vis = vis[:, track_input.prepend_length :]\n",
    "        conf = conf[:, track_input.prepend_length :]\n",
    "        print(tracks.shape, vis.shape, conf.shape)\n",
    "\n",
    "        pred_tracks_batch.append(tracks.cpu().numpy())\n",
    "        pred_visibility_batch.append(vis.cpu().numpy())\n",
    "        pred_confidence_batch.append(conf.cpu().numpy())\n",
    "        pred_poses_batch.append(poses.cpu().numpy())\n",
    "        pred_poses_err.append(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the results\n",
    "import pickle\n",
    "\n",
    "with open(tracker_result_video + \".pkl\", \"wb\") as f:\n",
    "    pickle.dump(\n",
    "        (\n",
    "            pred_tracks_batch,\n",
    "            pred_visibility_batch,\n",
    "            pred_confidence_batch,\n",
    "            pred_poses_batch,\n",
    "            pred_poses_err,\n",
    "            best_indices_batch,\n",
    "            track_input.canonical_points,\n",
    "            step,\n",
    "            overlap\n",
    "        ),\n",
    "        f,\n",
    "    )\n",
    "\n",
    "# Load the results\n",
    "\n",
    "with open(tracker_result_video + \".pkl\", \"rb\") as f:\n",
    "    (\n",
    "        pred_tracks_batch,\n",
    "        pred_visibility_batch,\n",
    "        pred_confidence_batch,\n",
    "        pred_poses_batch,\n",
    "        pred_poses_err,\n",
    "        best_indices_batch,\n",
    "        canonical_points,\n",
    "        step,\n",
    "        overlap\n",
    "    ) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reset_frame_range()\n",
    "N = len(dataset)\n",
    "video = load_video_images(tracker_input.video_dir)[:, -N:]\n",
    "# init_video = load_video_images(tracker_input.prepend_dir, limit=N, file_type=\"jpg\")\n",
    "video_original = load_video_images(dataset.video_rgb_dir)[:, -N:]\n",
    "# full_video = torch.cat([init_video, video_original], dim=1)[:, :N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_poses_batch)\n",
    "print(len(pred_poses_batch))\n",
    "print(pred_poses_batch[0].shape)\n",
    "pred_poses = []\n",
    "for i, pose_batch in enumerate(pred_poses_batch):\n",
    "    pred_poses.append(torch.tensor(pose_batch[int(i > 0) * overlap:]).float().to(device))\n",
    "epnp_slide_poses = torch.cat(pred_poses, dim=0).detach().cpu().numpy()\n",
    "print(epnp_slide_poses.shape)\n",
    "K = dataset.K\n",
    "camKs = torch.tensor(K[np.newaxis, :], device=device).float()\n",
    "gt_poses = torch.tensor(dataset.get_gt_poses()).float().to(device)\n",
    "print(gt_poses.shape)\n",
    "\n",
    "video_permuted = video_original[0].permute(0, 2, 3, 1)\n",
    "bbox_video = overlay_bounding_box_on_video(\n",
    "    video_permuted.detach().cpu().numpy(),\n",
    "    dataset.bbox.float(),\n",
    "    camKs.repeat(len(dataset), 1, 1).cpu(),\n",
    "    gt_poses.detach().cpu().numpy(),\n",
    ")\n",
    "bbox_video = overlay_bounding_box_on_video(\n",
    "    bbox_video,\n",
    "    dataset.bbox.float(),\n",
    "    camKs.repeat(len(dataset), 1, 1).cpu(),\n",
    "    epnp_slide_poses,\n",
    "    color=(255, 0, 0),\n",
    ")\n",
    "mediapy.show_video(bbox_video, fps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dataset)\n",
    "K = dataset.K\n",
    "x = (\n",
    "    torch.tensor(canonical_points, dtype=torch.float32)\n",
    "    .to(device)\n",
    "    .unsqueeze(0)\n",
    "    .repeat(N, 1, 1)\n",
    ")\n",
    "num_canonical_points = x.shape[1]\n",
    "\n",
    "best_tracks_slide = torch.zeros(1, N, num_canonical_points, 2).to(device)\n",
    "best_vis_slide = torch.zeros(1, N, num_canonical_points).to(device)\n",
    "best_conf_slide = torch.zeros(1, N, num_canonical_points).to(device)\n",
    "for i, coords, vis, conf, indices in zip(range(0, len(pred_tracks_batch)), pred_tracks_batch, pred_visibility_batch, pred_confidence_batch, best_indices_batch):\n",
    "    coords = torch.tensor(coords).float().to(device)\n",
    "    vis = torch.tensor(vis).float().to(device)\n",
    "    conf = torch.tensor(conf).float().to(device)\n",
    "    indices = torch.tensor(indices).int().to(device)\n",
    "    valid_indices = indices[indices >= 0]\n",
    "    invalid_indices_frames, invalid_indices_idx = torch.where(indices < 0)\n",
    "    left = i * (step - overlap)\n",
    "    right = min(left + step, dataset.max_frames)\n",
    "    frames_window = torch.arange(left, right, device=device).unsqueeze(-1)\n",
    "    if i > 0:\n",
    "        right_overlap = min(left + overlap, dataset.max_frames)\n",
    "        best_tracks_slide_overlap = best_tracks_slide[:, left:right_overlap]\n",
    "        best_vis_slide_overlap = best_vis_slide[:, left:right_overlap]\n",
    "        best_conf_slide_overlap = best_conf_slide[:, left:right_overlap]\n",
    "    best_tracks_slide[:, frames_window[:, 0]] = coords[:, frames_window - left, indices]\n",
    "    best_vis_slide[:, frames_window[:, 0]] = vis[:, frames_window - left, indices]\n",
    "    best_conf_slide[:, frames_window[:, 0]] = conf[:, frames_window - left, indices]\n",
    "    best_conf_slide[:, invalid_indices_frames + left, invalid_indices_idx] = 0\n",
    "    best_vis_slide[:, invalid_indices_frames + left, invalid_indices_idx] = 0\n",
    "    if i > 0:\n",
    "        best_tracks_slide[:, left:right_overlap] = best_tracks_slide_overlap\n",
    "        best_vis_slide[:, left:right_overlap] = best_vis_slide_overlap\n",
    "        best_conf_slide[:, left:right_overlap] = best_conf_slide_overlap\n",
    "        \n",
    "    \n",
    "\n",
    "visualize_results(\n",
    "    video,\n",
    "    best_tracks_slide,\n",
    "    best_vis_slide,\n",
    "    best_conf_slide,\n",
    "    tracker_result_video + \"_slide\",\n",
    "    num_of_main_queries=num_canonical_points,\n",
    ")\n",
    "print(num_canonical_points, tracker_input.num_canonical_points)\n",
    "\n",
    "\n",
    "y_slide = unscale_by_crop(\n",
    "        best_tracks_slide[0],\n",
    "        torch.tensor(bboxes).to(device),\n",
    "        torch.tensor(scaling).to(device),\n",
    "    )\n",
    "\n",
    "# y = best_tracks_slide.detach().clone().squeeze(0)\n",
    "init_poses = torch.tensor(epnp_slide_poses, device=device).float()[:N]\n",
    "\n",
    "weights_slide = (best_vis_slide * best_conf_slide).float()\n",
    "weights_slide[best_vis_slide * best_conf_slide < VIS_CONF_THRESHOLD] = 0\n",
    "weights_slide = weights_slide.squeeze(0)\n",
    "   \n",
    "print(x.shape, camKs.shape, init_poses.shape, y_slide.shape, weights_slide.shape)\n",
    "gradient_pnp = GradientPnP(\n",
    "    max_lr=0.02,\n",
    "    temporal_consistency_weight=1,\n",
    "    X=x[0],\n",
    "    K=camKs,\n",
    ")\n",
    "\n",
    "rotations, translations, all_results = gradient_pnp(\n",
    "    y_slide,\n",
    "    weights=weights_slide,\n",
    "    R=init_poses[:, :3, :3],\n",
    "    T=init_poses[:, :3, 3],\n",
    ")\n",
    "\n",
    "gradient_poses_slide = torch.eye(4).to(device).unsqueeze(0).repeat(N, 1, 1)\n",
    "gradient_poses_slide[:, :3, :3] = rotations\n",
    "gradient_poses_slide[:, :3, 3] = translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from posingpixels.pointselector import SelectMostConfidentView\n",
    "\n",
    "\n",
    "true_indexes = torch.tensor(tracker_input.query_to_point_indexes, device=device)\n",
    "query_lengths = torch.tensor(tracker_input.query_lengths, device=device)\n",
    "\n",
    "# tensor_query_to_point = torch.tensor(tracker.query_to_point, device=device)\n",
    "# true_indexes = torch.nonzero(tensor_query_to_point)\n",
    "# query_lengths = torch.tensor(tracker.queries_sizes, device=device)\n",
    "\n",
    "# point_selector = SelectMostConfidentPoint(\n",
    "#     tracker_input.num_canonical_points, true_indexes, query_lengths\n",
    "# )\n",
    "point_selector = SelectMostConfidentView(\n",
    "    tracker_input.num_canonical_points, true_indexes, query_lengths\n",
    ")\n",
    "\n",
    "best_coords, best_vis, best_conf, best_indices = point_selector.query_to_point(\n",
    "    pred_tracks[0],\n",
    "    pred_visibility[0],\n",
    "    pred_confidence[0],\n",
    "    # pred_visibility[0] * pred_confidence[0],\n",
    ")\n",
    "best_coords = best_coords.unsqueeze(0)\n",
    "best_vis = best_vis.unsqueeze(0)\n",
    "best_conf = best_conf.unsqueeze(0)\n",
    "\n",
    "best_coords_original = unscale_by_crop(\n",
    "    best_coords[0],\n",
    "    torch.tensor(tracker_input.bboxes).to(device),\n",
    "    torch.tensor(tracker_input.scaling).to(device),\n",
    ").unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(\n",
    "    video,\n",
    "    pred_tracks[:, tracker_input.prepend_length :],\n",
    "    pred_visibility[:, tracker_input.prepend_length :],\n",
    "    pred_confidence[:, tracker_input.prepend_length :],\n",
    "    tracker_result_video,\n",
    "    num_of_main_queries=Q,\n",
    ")\n",
    "\n",
    "visualize_results(\n",
    "    video,\n",
    "    best_coords[:, tracker_input.prepend_length :],\n",
    "    best_vis[:, tracker_input.prepend_length :],\n",
    "    best_conf[:, tracker_input.prepend_length :],\n",
    "    tracker_result_video,\n",
    "    filename=\"selected_video\",\n",
    ")\n",
    "\n",
    "gt_tracks, gt_visibility = get_gt_tracks(tracker_input)\n",
    "visualize_results(\n",
    "    video,\n",
    "    torch.tensor(gt_tracks).to(device).unsqueeze(0).float()[:, tracker_input.prepend_length :],\n",
    "    torch.tensor(gt_visibility).to(device).unsqueeze(0).float()[:, tracker_input.prepend_length :],\n",
    "    torch.ones_like(torch.tensor(gt_visibility).to(device)).unsqueeze(0).float()[:, tracker_input.prepend_length :],\n",
    "    tracker_result_video,\n",
    "    num_of_main_queries=Q,\n",
    "    filename=\"gt_video\",\n",
    "    threshold=VIS_CONF_THRESHOLD,\n",
    ")\n",
    "# pred_tracks_original = unscale_by_crop(\n",
    "#     pred_tracks[0],\n",
    "#     torch.tensor(tracker_input.bboxes).to(device),\n",
    "#     torch.tensor(tracker_input.scaling).to(device),\n",
    "# ).unsqueeze(0)\n",
    "\n",
    "# full_video = torch.cat([init_video, video_original], dim=1)[:, : N]\n",
    "# visualize_results(\n",
    "#     full_video,\n",
    "#     pred_tracks_original,\n",
    "#     pred_visibility,\n",
    "#     pred_confidence,\n",
    "#     tracker_result_video,\n",
    "#     num_of_main_queries=Q,\n",
    "#     filename=\"original\",\n",
    "#     threshold=VIS_CONF_THRESHOLD,\n",
    "# )\n",
    "\n",
    "# gt_tracks_original, gt_visibility_original = get_gt_tracks(tracker_input, crop=False)\n",
    "# visualize_results(\n",
    "#     full_video,\n",
    "#     torch.tensor(gt_tracks_original).to(device).unsqueeze(0).float(),\n",
    "#     torch.tensor(gt_visibility_original).to(device).unsqueeze(0).float(),\n",
    "#     torch.ones_like(torch.tensor(gt_visibility_original).to(device))\n",
    "#     .unsqueeze(0)\n",
    "#     .float(),\n",
    "#     tracker_result_video,\n",
    "#     num_of_main_queries=Q,\n",
    "#     filename=\"gt_original\",\n",
    "#     threshold=VIS_CONF_THRESHOLD,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_points = tracker_input.input_query[\n",
    "    np.newaxis, : tracker_input.num_canonical_points\n",
    "]\n",
    "print(query_points.shape)\n",
    "gt_occluded = (gt_visibility.T < 0.5)[np.newaxis, :]\n",
    "print(gt_occluded.shape)\n",
    "gt_tracks_ = np.transpose(gt_tracks[np.newaxis, :], (0, 2, 1, 3))\n",
    "print(gt_tracks_.shape)\n",
    "pred_visibility_ = best_vis.permute(0, 2, 1).cpu().numpy()\n",
    "pred_occluded = pred_visibility_ < 0.5\n",
    "print(pred_occluded.shape)\n",
    "pred_tracks_ = best_coords.permute(0, 2, 1, 3).cpu().numpy()\n",
    "print(pred_tracks_.shape)\n",
    "pred_confidence_ = best_conf.cpu().permute(0, 2, 1).numpy()\n",
    "print(pred_confidence_.shape)\n",
    "\n",
    "# We are only interested in evaluating points where confidence * visibility > 0.6 (B x N x T)\n",
    "threshold = VIS_CONF_THRESHOLD\n",
    "evaluation_points = pred_confidence_ * pred_visibility_ > threshold\n",
    "# evaluation_points = np.ones_like(pred_confidence_, dtype=bool)\n",
    "\n",
    "metrics = compute_tapvid_metrics(\n",
    "    query_points=query_points,\n",
    "    gt_occluded=gt_occluded,\n",
    "    gt_tracks=gt_tracks_,\n",
    "    pred_occluded=pred_occluded,\n",
    "    pred_tracks=pred_tracks_,\n",
    "    query_mode=\"first\",\n",
    "    evaluation_points=evaluation_points,\n",
    ")\n",
    "# Print the following metrics\n",
    "print(\"occlusion_accuracy\", metrics[\"occlusion_accuracy\"])\n",
    "print(\"average_jaccard\", metrics[\"average_jaccard\"])\n",
    "print(\"average_pts_within_thresh\", metrics[\"average_pts_within_thresh\"])\n",
    "print(\"pts_within_1\", metrics[\"pts_within_1\"])\n",
    "print(\"jaccard_1\", metrics[\"jaccard_1\"])\n",
    "print(\"pts_within_2\", metrics[\"pts_within_2\"])\n",
    "print(\"jaccard_2\", metrics[\"jaccard_2\"])\n",
    "print(\"pts_within_4\", metrics[\"pts_within_4\"])\n",
    "print(\"jaccard_4\", metrics[\"jaccard_4\"])\n",
    "print(\"pts_within_8\", metrics[\"pts_within_8\"])\n",
    "print(\"jaccard_8\", metrics[\"jaccard_8\"])\n",
    "print(\"pts_within_16\", metrics[\"pts_within_16\"])\n",
    "print(\"jaccard_16\", metrics[\"jaccard_16\"])\n",
    "print(metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many points are being evaluated per time step?\n",
    "evaluation_points.shape\n",
    "plt.plot(evaluation_points.sum(axis=(0, 1)))\n",
    "plt.title(\n",
    "    f\"Number of points considered per time step with visibility * confidence > {threshold}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"per_point_pts_within_8\"\n",
    "values = metrics[metric_name][0]\n",
    "# If we're doing a time metric, plot a line plot\n",
    "# If we're doing a per point metric, plot distribution of values\n",
    "plt.plot(values) if \"time\" in metric_name else plt.hist(values, bins=20)\n",
    "# Plot vertical line on self.interpolation_steps\n",
    "plt.axvline(\n",
    "    tracker_input.prepend_length, color=\"r\", linestyle=\"--\"\n",
    ") if \"time\" in metric_name else None\n",
    "plt.title(f\"{metric_name} (threshold={threshold})\")\n",
    "plt.xlabel(\"Frame\" if \"time\" in metric_name else \"Point\")\n",
    "plt.ylabel(\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========\n",
    "# Input\n",
    "# ==========\n",
    "import time\n",
    "\n",
    "import tqdm\n",
    "from posingpixels.pnp import OpenCVePnP\n",
    "from posingpixels.query_refiner import QueryRefiner\n",
    "tracker_input.dataset.reset_frame_range()\n",
    "\n",
    "K = tracker_input.dataset.K\n",
    "x = (\n",
    "    torch.tensor(tracker_input.canonical_points, dtype=torch.float32)\n",
    "    .to(device)\n",
    "    .unsqueeze(0)\n",
    "    .repeat(N, 1, 1)\n",
    ")\n",
    "gt_poses = torch.tensor(tracker_input.gt_poses[tracker_input.prepend_length: tracker_input.prepend_length + N]).float().to(device)\n",
    "gt_posed_x = apply_pose_to_points_batch(x, gt_poses[:, :3, :3], gt_poses[:, :3, 3])\n",
    "y_gt = render_points_in_2d_batch(gt_posed_x, torch.tensor(K[:3, :3]).float().to(device))\n",
    "\n",
    "y = best_coords_original.detach().clone().squeeze(0)[tracker_input.prepend_length: tracker_input.prepend_length + N]\n",
    "\n",
    "\n",
    "weights = (best_vis * best_conf).float()\n",
    "weights[best_vis * best_conf < VIS_CONF_THRESHOLD] = 0\n",
    "weights = weights.squeeze(0)[tracker_input.prepend_length: tracker_input.prepend_length + N]\n",
    "\n",
    "camKs = torch.tensor(K[np.newaxis, :], device=device).float()\n",
    "\n",
    "# ==========\n",
    "# ePnP\n",
    "# ==========\n",
    "# epnp_solver = RANSACePnP(num_iterations=100)\n",
    "# epnp_solver = ePnP()\n",
    "# epnp_R, epnp_T, _ = epnp_solver(\n",
    "#     y,\n",
    "#     X=x,\n",
    "#     K=torch.tensor(K).to(device).float(),  # weights=weights\n",
    "# )\n",
    "# epnp_poses = torch.eye(4).to(device).unsqueeze(0).repeat(N, 1, 1)\n",
    "# epnp_poses[:, :3, :3] = epnp_R\n",
    "# epnp_poses[:, :3, 3] = epnp_T\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "epnp_cv_solver = OpenCVePnP(\n",
    "    X=x[0],\n",
    "    K=camKs,\n",
    "    ransac_iterations=5000,\n",
    "    ransac_inliner_threshold=2.0,\n",
    ")\n",
    "epnp_cv_R, epnp_cv_T, _ = epnp_cv_solver(\n",
    "    y,\n",
    "    # X=x,\n",
    "    K=torch.tensor(K).to(device).float(), weights=weights,\n",
    ")\n",
    "epnp_cv_poses = torch.eye(4).to(device).unsqueeze(0).repeat(N, 1, 1)\n",
    "epnp_cv_poses[:, :3, :3] = epnp_cv_R\n",
    "epnp_cv_poses[:, :3, 3] = epnp_cv_T\n",
    "# End time\n",
    "end_time = time.time()\n",
    "print(f\"Time to run OpenCV ePnP: {end_time - start_time}\")\n",
    "\n",
    "# ==========\n",
    "# Our Model Complete\n",
    "# ==========\n",
    "# query_refiner = QueryRefiner(\n",
    "#     point_selector,\n",
    "#     epnp_cv_solver,\n",
    "#     torch.tensor(tracker_input.bboxes, device=device),\n",
    "#     torch.tensor(tracker_input.scaling, device=device),\n",
    "#     gt_poses[: tracker_input.prepend_length, :3, :3],\n",
    "#     gt_poses[: tracker_input.prepend_length, :3, 3],\n",
    "#     threshold=0.9,\n",
    "#     pad_inputs=False,\n",
    "# )\n",
    "# coordinates = pred_tracks[0, :N].clone()\n",
    "# visibility = pred_visibility[0, :N].clone()\n",
    "# confidence = pred_confidence[0, :N].clone()\n",
    "# optimization_results_query_refiner = []\n",
    "# for i in tqdm.tqdm(range(N // query_refiner.step)):\n",
    "#     if query_refiner.current >= N:\n",
    "#         break\n",
    "#     left = i * query_refiner.step\n",
    "#     right = min((i + 1) * query_refiner.step + query_refiner.step, N)\n",
    "#     (\n",
    "#         coordinates[left:right],\n",
    "#         visibility[left:right],\n",
    "#         confidence[left:right],\n",
    "#         opt_results,\n",
    "#     ) = query_refiner(\n",
    "#         coordinates[left:right], visibility[left:right], confidence[left:right]\n",
    "#     )\n",
    "#     optimization_results_query_refiner.append(opt_results)\n",
    "# query_refiner_poses = torch.eye(4).to(device).unsqueeze(0).repeat(N, 1, 1)\n",
    "# query_refiner_poses[:, :3, :3] = query_refiner.R\n",
    "# query_refiner_poses[:, :3, 3] = query_refiner.T\n",
    "\n",
    "# ==========\n",
    "# Our model\n",
    "# ==========\n",
    "\n",
    "gradient_pnp = GradientPnP(\n",
    "    max_lr=0.02,\n",
    "    temporal_consistency_weight=1,\n",
    "    X=x[0],\n",
    "    K=camKs,\n",
    "    # R=gt_poses[0, :3, :3],\n",
    "    # T=gt_poses[0, :3, 3],\n",
    ")\n",
    "\n",
    "rotations, translations, all_results = gradient_pnp(\n",
    "    y,\n",
    "    weights=weights,\n",
    "    # R=gt_poses[0, :3, :3],\n",
    "    # T=gt_poses[0, :3, 3],\n",
    "    # R=torch.eye(3).to(device),\n",
    "    # T=torch.zeros(3).to(device),\n",
    "    # R=query_refiner.R.clone(),\n",
    "    # T=query_refiner.T.clone(),\n",
    "    R=epnp_cv_poses[:, :3, :3].clone(),\n",
    "    T=epnp_cv_poses[:, :3, 3].clone(),\n",
    ")\n",
    "\n",
    "gradient_poses = torch.eye(4).to(device).unsqueeze(0).repeat(N, 1, 1)\n",
    "gradient_poses[:, :3, :3] = rotations\n",
    "gradient_poses[:, :3, 3] = translations\n",
    "\n",
    "\n",
    "# ==========\n",
    "# Visualize\n",
    "# ==========\n",
    "\n",
    "my_predicted_poses = gradient_poses\n",
    "# my_predicted_poses = epnp_poses\n",
    "\n",
    "# video_permuted = full_video[0].permute(0, 2, 3, 1)\n",
    "# bbox_video = overlay_bounding_box_on_video(\n",
    "#     video_permuted[:N].detach().cpu().numpy(),\n",
    "#     dataset.bbox.float(),\n",
    "#     camKs.repeat(N, 1, 1).cpu(),\n",
    "#     gt_poses.detach().cpu().numpy(),\n",
    "# )\n",
    "# bbox_video = overlay_bounding_box_on_video(\n",
    "#     bbox_video,\n",
    "#     dataset.bbox.float(),\n",
    "#     camKs.repeat(N, 1, 1).cpu(),\n",
    "#     my_predicted_poses.detach().cpu().numpy(),\n",
    "#     color=(255, 0, 0),\n",
    "# )\n",
    "# mediapy.show_video(bbox_video[:N], fps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_refiner_poses = torch.eye(4).to(device).unsqueeze(0).repeat(N, 1, 1)\n",
    "# query_refiner_poses[:, :3, :3] = query_refiner.R\n",
    "# query_refiner_poses[:, :3, 3] = query_refiner.T\n",
    "\n",
    "my_predicted_poses = gradient_poses\n",
    "\n",
    "video_permuted = video_original[0].permute(0, 2, 3, 1)\n",
    "bbox_video = overlay_bounding_box_on_video(\n",
    "    video_permuted[:N].detach().cpu().numpy(),\n",
    "    dataset.bbox.float(),\n",
    "    camKs.repeat(N, 1, 1).cpu(),\n",
    "    gt_poses.detach().cpu().numpy(),\n",
    ")\n",
    "bbox_video = overlay_bounding_box_on_video(\n",
    "    bbox_video,\n",
    "    dataset.bbox.float(),\n",
    "    camKs.repeat(N, 1, 1).cpu(),\n",
    "    my_predicted_poses.detach().cpu().numpy(),\n",
    "    color=(255, 0, 0),\n",
    ")\n",
    "mediapy.show_video(bbox_video[:N], fps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_predicted_poses = epnp_cv_poses\n",
    "\n",
    "video_permuted = video_original[0].permute(0, 2, 3, 1)\n",
    "bbox_video = overlay_bounding_box_on_video(\n",
    "    video_permuted[:N].detach().cpu().numpy(),\n",
    "    dataset.bbox.float(),\n",
    "    camKs.repeat(N, 1, 1).cpu(),\n",
    "    gt_poses.detach().cpu().numpy(),\n",
    ")\n",
    "bbox_video = overlay_bounding_box_on_video(\n",
    "    bbox_video,\n",
    "    dataset.bbox.float(),\n",
    "    camKs.repeat(N, 1, 1).cpu(),\n",
    "    my_predicted_poses.detach().cpu().numpy(),\n",
    "    color=(255, 0, 0),\n",
    ")\n",
    "mediapy.show_video(bbox_video[:N], fps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_plot_add_metrics(\n",
    "    model_3D_pts,\n",
    "    diameter,\n",
    "    predicted_poses: np.ndarray,\n",
    "    gt_poses: np.ndarray,\n",
    "    percentage=0.1,\n",
    "    vert_lines=[],\n",
    "):\n",
    "    add_metrics = []\n",
    "    for i in range(predicted_poses.shape[0]):\n",
    "        add_metrics.append(\n",
    "            compute_add_metrics(\n",
    "                model_3D_pts,\n",
    "                diameter,\n",
    "                predicted_poses[i],\n",
    "                gt_poses[i],\n",
    "                percentage=percentage,\n",
    "                return_error=True,\n",
    "            )\n",
    "        )\n",
    "    threshold = diameter * percentage\n",
    "    print(\n",
    "        f\"Percentage of ADD error less than {threshold}: {np.mean(np.array(add_metrics) < threshold, axis=0)}\"\n",
    "    )\n",
    "    print(f\"Mean ADD error: {np.mean(add_metrics)}\")\n",
    "    plt.plot(add_metrics)\n",
    "    plt.axhline(threshold, color=\"r\", linestyle=\"--\")\n",
    "    for vert_line in vert_lines:\n",
    "        plt.axvline(vert_line, color=\"g\", linestyle=\"--\")\n",
    "    plt.title(\"ADD Error over time\")\n",
    "    plt.xlabel(\"Frame\")\n",
    "    plt.ylabel(\"ADD Error\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"RANSAC CV ePnP\")\n",
    "compute_and_plot_add_metrics(\n",
    "    np.array(dataset.get_mesh().vertices),\n",
    "    dataset.obj_diameter,\n",
    "    epnp_slide_poses,\n",
    "    gt_poses.detach().cpu().numpy(), # [tracker_input.prepend_length :],\n",
    "    percentage=0.1,\n",
    "    vert_lines=[i * (step - overlap) + overlap * int(i > 0) for i in range(N // (step - overlap))],\n",
    ")\n",
    "print(\"Adam Optimizer\")\n",
    "compute_and_plot_add_metrics(\n",
    "    np.array(dataset.get_mesh().vertices),\n",
    "    dataset.obj_diameter,\n",
    "    gradient_poses_slide.detach().cpu().numpy(),#[tracker_input.prepend_length :],\n",
    "    gt_poses.detach().cpu().numpy(),#[tracker_input.prepend_length :],\n",
    "    percentage=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"RANSAC CV ePnP\")\n",
    "compute_and_plot_add_metrics(\n",
    "    np.array(dataset.get_mesh().vertices),\n",
    "    dataset.obj_diameter,\n",
    "    epnp_cv_poses.detach().cpu().numpy()[:],\n",
    "    gt_poses.detach().cpu().numpy(),\n",
    "    percentage=0.1,\n",
    ")\n",
    "print(\"Adam Optimizer\")\n",
    "compute_and_plot_add_metrics(\n",
    "    np.array(dataset.get_mesh().vertices),\n",
    "    dataset.obj_diameter,\n",
    "    gradient_poses.detach().cpu().numpy()[:],\n",
    "    gt_poses.detach().cpu().numpy(),\n",
    "    percentage=0.1,\n",
    ")\n",
    "# print(\"QueryRefiner Optimizer\")\n",
    "# compute_and_plot_add_metrics(\n",
    "#     np.array(dataset.get_mesh().vertices),\n",
    "#     dataset.obj_diameter,\n",
    "#     query_refiner_poses.detach().cpu().numpy()[tracker_input.prepend_length :],\n",
    "#     gt_poses.detach().cpu().numpy()[tracker_input.prepend_length :],\n",
    "#     percentage=0.1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "sift = cv.SIFT_create()\n",
    "rgb, _, _ = dataset.render_mesh_at_pose(dataset.get_canonical_pose())\n",
    "kp = sift.detect(rgb, None)\n",
    "\n",
    "img = cv.drawKeypoints(rgb, kp, None)\n",
    "plt.imshow(img)\n",
    "kp_np = np.array([k.pt for k in kp])\n",
    "print(kp)\n",
    "print(kp_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw distribution of confidence (in bins)\n",
    "# pred_confidence\n",
    "plt.hist(pred_confidence[0].cpu().numpy().flatten(), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gspose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
